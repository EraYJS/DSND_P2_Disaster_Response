{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline \n",
    "按照如下的指导要求，搭建你的机器学习管道。\n",
    "### 1. 导入与加载\n",
    "- 导入 Python 库\n",
    "- 使用 [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html) 从数据库中加载数据集\n",
    "- 定义特征变量X 和目标变量 Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   id                                            message  \\\n0   2  Weather update - a cold front from Cuba that c...   \n1   7            Is the Hurricane over or is it not over   \n2   8                    Looking for someone but no name   \n3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n4  12  says: west side of Haiti, rest of the country ...   \n\n                                            original   genre  related  \\\n0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n1                 Cyclone nan fini osinon li pa fini  direct        1   \n2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n\n   request  offer  aid_related  medical_help  medical_products  ...  \\\n0        0      0            0             0                 0  ...   \n1        0      0            1             0                 0  ...   \n2        0      0            0             0                 0  ...   \n3        1      0            1             0                 1  ...   \n4        0      0            0             0                 0  ...   \n\n   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n0            0                     0                0       0      0     0   \n1            0                     0                1       0      1     0   \n2            0                     0                0       0      0     0   \n3            0                     0                0       0      0     0   \n4            0                     0                0       0      0     0   \n\n   earthquake  cold  other_weather  direct_report  \n0           0     0              0              0  \n1           0     0              0              0  \n2           0     0              0              0  \n3           0     0              0              0  \n4           0     0              0              0  \n\n[5 rows x 40 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>message</th>\n      <th>original</th>\n      <th>genre</th>\n      <th>related</th>\n      <th>request</th>\n      <th>offer</th>\n      <th>aid_related</th>\n      <th>medical_help</th>\n      <th>medical_products</th>\n      <th>...</th>\n      <th>aid_centers</th>\n      <th>other_infrastructure</th>\n      <th>weather_related</th>\n      <th>floods</th>\n      <th>storm</th>\n      <th>fire</th>\n      <th>earthquake</th>\n      <th>cold</th>\n      <th>other_weather</th>\n      <th>direct_report</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Weather update - a cold front from Cuba that c...</td>\n      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n      <td>direct</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>Is the Hurricane over or is it not over</td>\n      <td>Cyclone nan fini osinon li pa fini</td>\n      <td>direct</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>Looking for someone but no name</td>\n      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n      <td>direct</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n      <td>direct</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>says: west side of Haiti, rest of the country ...</td>\n      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n      <td>direct</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///data/disaster_sn_msg.db')\n",
    "data = pd.read_sql_table(\"data\", engine)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "X = data['message']\n",
    "Y = data.iloc[:, 4:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 编写分词函数，开始处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    #tokenize\n",
    "    words = word_tokenize (text)\n",
    "\n",
    "    #stemming\n",
    "    stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "    #lemmatizing\n",
    "    words_lemmed = [WordNetLemmatizer().lemmatize(w) for w in stemmed if w not in stop_words]\n",
    "\n",
    "    return words_lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 创建机器学习管道 \n",
    "这个机器学习管道应该接收 `message` 列作输入，输出分类结果，分类结果属于该数据集中的 36 个类。你会发现 [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) 在预测多目标变量时很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ppl():\n",
    "    ppl = pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier (AdaBoostClassifier()))\n",
    "        ])\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "pipeline = build_ppl()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 训练管道\n",
    "- 将数据分割成训练和测试集\n",
    "- 训练管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function tokenize at 0x7fe5e9276710>,\n                                 vocabulary=None)),\n                ('tfidf',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n                                                                    base_estimator=None,\n                                                                    learning_rate=1.0,\n                                                                    n_estimators=50,\n                                                                    random_state=None),\n                                       n_jobs=None))],\n         verbose=False)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 测试模型\n",
    "报告数据集中每个输出类别的 f1 得分、准确度和召回率。你可以对列进行遍历，并对每个元素调用 sklearn 的 `classification_report`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def perf_eval(y_test_df, y_pred_np):\n",
    "    \"\"\"\n",
    "    This is a customized evaluation function that measures both the label recall\n",
    "    for each sample and the precision of each label across all samples.\n",
    "    :param y_test_df: ground truth\n",
    "    :param y_pred_np: predictions\n",
    "    :return: sample_label_recall: the label recall for each sample\n",
    "             label_precision: the precision of each label across all samples\n",
    "             f1_score: the f1 score calculated from the mean of the above two\n",
    "    \"\"\"\n",
    "    y_test_np = np.array(y_test_df)\n",
    "    sample_label_recall = []\n",
    "    label_precision = []\n",
    "\n",
    "    for i in range(y_test_df.shape[0]):\n",
    "        if y_test_np[i].sum() != 0:\n",
    "            sample_label_recall.append(np.bitwise_and(\n",
    "                    y_pred_np[i], y_test_np[i]).sum() / y_test_np[i].sum())\n",
    "        elif y_test_np[i].sum() == 0:\n",
    "            sample_label_recall.append(1)\n",
    "\n",
    "    for j in range(y_test_np.shape[1]):\n",
    "        label_precision.append(np.invert(\n",
    "                np.logical_xor(y_pred_np[:, j],\n",
    "                               y_test_np[:, j])).sum() / y_test_np.shape[0])\n",
    "\n",
    "    slr_mean = np.array(sample_label_recall).mean()\n",
    "    lp_mean = np.array(label_precision).mean()\n",
    "    f1_score = 2 * slr_mean * lp_mean / (slr_mean + lp_mean)\n",
    "\n",
    "    label_precision_df = pd.DataFrame(label_precision).transpose()\n",
    "    label_precision_df.columns = y_test_df.columns\n",
    "\n",
    "    report = pd.DataFrame()\n",
    "    y_pred_df = pd.DataFrame(y_pred_np)\n",
    "    y_pred_df.columns = y_test_df.columns\n",
    "\n",
    "    for col in y_test_df.columns:\n",
    "        class_dict = classification_report(output_dict=True,\n",
    "                                           y_true=y_test_df.loc[:, col],\n",
    "                                           y_pred=y_pred_df.loc[:, col])\n",
    "\n",
    "        eval_df = pd.DataFrame(pd.DataFrame.from_dict(class_dict))\n",
    "\n",
    "        # dropping unnecessary information\n",
    "        eval_df.drop(['macro avg', 'weighted avg'], axis=1,\n",
    "                     inplace=True)\n",
    "        eval_df.drop(index='support', inplace=True)\n",
    "\n",
    "        av_eval_df = pd.DataFrame(eval_df.transpose().mean()).transpose()\n",
    "\n",
    "        report = report.append(av_eval_df, ignore_index=True)\n",
    "\n",
    "    report.index = y_test_df.columns\n",
    "\n",
    "    return slr_mean, lp_mean, label_precision_df, f1_score, report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred):\n",
    "    slr_mean, lp_mean, label_precision_df, f1_score, report = \\\n",
    "            perf_eval(y_test, y_pred)\n",
    "\n",
    "    print(f'Custom F1-Score is {f1_score}.')\n",
    "    print(f'The mean of each label precision  is {lp_mean}.')\n",
    "    print(f'The mean of label recall for each sample is {slr_mean}.')\n",
    "\n",
    "    print('=======================================')\n",
    "    print('Printing custom each label precision...')\n",
    "    print(label_precision_df)\n",
    "\n",
    "    print('=======================================')\n",
    "    print('Printing classification report...')\n",
    "    print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom F1-Score is 0.8432344244940265.\n",
      "The mean of custom precision above is 0.9472729490933376.\n",
      "The mean of label recall for each sample is 0.7597873513753752.\n",
      "=======================================\n",
      "Printing custom each label precision...\n",
      "    related   request     offer  aid_related  medical_help  medical_products  \\\n",
      "0  0.777393  0.891123  0.994799     0.765025      0.927184          0.958622   \n",
      "\n",
      "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
      "0           0.975035  0.981392  0.971567          1.0  ...     0.986246   \n",
      "\n",
      "   other_infrastructure  weather_related    floods     storm      fire  \\\n",
      "0              0.951341         0.876791  0.958969  0.938049  0.988673   \n",
      "\n",
      "   earthquake      cold  other_weather  direct_report  \n",
      "0    0.969371  0.983934       0.946602       0.851017  \n",
      "\n",
      "[1 rows x 36 columns]\n",
      "=======================================\n",
      "Printing classification report...\n",
      "                        precision    recall  f1-score\n",
      "related                  0.628270  0.499552  0.503328\n",
      "request                  0.854891  0.797650  0.819288\n",
      "offer                    0.693886  0.673805  0.678248\n",
      "aid_related              0.764200  0.752952  0.756023\n",
      "medical_help             0.826325  0.725404  0.752429\n",
      "medical_products         0.854582  0.750366  0.782141\n",
      "search_and_rescue        0.835481  0.719500  0.747444\n",
      "security                 0.788156  0.681243  0.694162\n",
      "military                 0.849407  0.761508  0.791352\n",
      "child_alone              1.000000  1.000000  1.000000\n",
      "water                    0.892862  0.856999  0.873206\n",
      "food                     0.902357  0.863807  0.880925\n",
      "shelter                  0.887089  0.818990  0.845950\n",
      "clothing                 0.895276  0.837906  0.862395\n",
      "money                    0.825829  0.755313  0.780486\n",
      "missing_people           0.826085  0.683314  0.697746\n",
      "refugees                 0.847576  0.732746  0.763494\n",
      "death                    0.883971  0.793537  0.826317\n",
      "other_aid                0.758217  0.664980  0.675335\n",
      "infrastructure_related   0.789531  0.678913  0.693558\n",
      "transport                0.851262  0.718382  0.747837\n",
      "buildings                0.852887  0.782820  0.809385\n",
      "electricity              0.818673  0.747647  0.772328\n",
      "tools                    0.713746  0.677001  0.684028\n",
      "hospitals                0.826854  0.700302  0.722675\n",
      "shops                    0.774386  0.679891  0.690672\n",
      "aid_centers              0.761703  0.689079  0.703567\n",
      "other_infrastructure     0.758643  0.679166  0.691930\n",
      "weather_related          0.873089  0.834692  0.849344\n",
      "floods                   0.931711  0.839557  0.874746\n",
      "storm                    0.882204  0.812137  0.839402\n",
      "fire                     0.847790  0.733426  0.764274\n",
      "earthquake               0.940826  0.913593  0.926344\n",
      "cold                     0.902485  0.787139  0.825854\n",
      "other_weather            0.821685  0.706300  0.730823\n",
      "direct_report            0.805423  0.741799  0.761725\n"
     ]
    }
   ],
   "source": [
    "print_metrics(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 优化模型\n",
    "使用网格搜索来找到最优的参数组合。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_eval(y_test, y_pred):\n",
    "    ytest = np.array(y_test)\n",
    "    sample_label_recall = []\n",
    "    label_precision = []\n",
    "\n",
    "    for i in range(y_test.shape[0]):\n",
    "        if ytest[i].sum() != 0:\n",
    "            sample_label_recall.append(np.bitwise_and(\n",
    "                    y_pred[i], ytest[i]).sum() / ytest[i].sum())\n",
    "        elif ytest[i].sum() == 0:\n",
    "            sample_label_recall.append(1)\n",
    "\n",
    "    for j in range(y_test.shape[1]):\n",
    "        label_precision.append(np.invert(\n",
    "                np.logical_xor(y_pred[:, j],\n",
    "                               ytest[:, j])).sum() \\\n",
    "                               / ytest.shape[0])\n",
    "\n",
    "    slr_mean = np.array(sample_label_recall).mean()\n",
    "    lp_mean = np.array(label_precision).mean()\n",
    "    f1_score = 2 * slr_mean * lp_mean / (slr_mean + lp_mean)\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "scorer = make_scorer(gs_eval)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "parameters = {'vect__max_df': (0.3, 0.4, 0.5),\n",
    "              'clf__estimator__n_estimators': range(50, 54, 1),\n",
    "              'clf__estimator__learning_rate': np.arange(1.2, 2.0, 0.2)\n",
    "              }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-27ea17616388>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m gs = GridSearchCV(pipeline,\n\u001B[0m\u001B[1;32m      2\u001B[0m                   \u001B[0mparam_grid\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                   \u001B[0mscoring\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mscorer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                   \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m16\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                   \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline,\n",
    "                  param_grid=parameters,\n",
    "                  scoring=scorer,\n",
    "                  n_jobs=16,\n",
    "                  verbose=7\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "gs.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'clf__estimator__learning_rate': 1.2,\n 'clf__estimator__n_estimators': 51,\n 'vect__max_df': 0.4}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 测试模型\n",
    "打印微调后的模型的精确度、准确率和召回率。  \n",
    "\n",
    "因为本项目主要关注代码质量、开发流程和管道技术，所有没有模型性能指标的最低要求。但是，微调模型提高精确度、准确率和召回率可以让你的项目脱颖而出——特别是让你的简历更出彩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gs = gs.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 继续优化模型，比如：\n",
    "* 尝试其他的机器学习算法\n",
    "* 尝试除 TF-IDF 外其他的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add start verb feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    @staticmethod\n",
    "    def starting_verb(text):\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = pos_tag(tokenize(sentence))\n",
    "            try:\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return True\n",
    "            except IndexError:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add XGBoost as classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_ppl(clf_type):\n",
    "    \"\"\"\n",
    "    Grid Search Results:\n",
    "        AdaBoost:\n",
    "            {'clf__estimator__learning_rate': 1.2,\n",
    "             'clf__estimator__n_estimators': 51,\n",
    "             'vect__max_df': 0.4}\n",
    "\n",
    "\n",
    "        XGBoost:\n",
    "            {'clf__estimator__colsample_bytree': 1.0,\n",
    "             'clf__estimator__gamma': 5.0,\n",
    "             'clf__estimator__learning_rate': 0.5,\n",
    "             'clf__estimator__min_child_weight': 1,\n",
    "             'clf__estimator__subsample': 1.0,\n",
    "             'vect__max_df': 0.75}\n",
    "    \"\"\"\n",
    "\n",
    "    if clf_type == \"Ada\":\n",
    "\n",
    "        ada_ppl = Pipeline([\n",
    "                ('feats', FeatureUnion([\n",
    "                        ('text_ppl', Pipeline([\n",
    "                                ('vect', CountVectorizer(\n",
    "                                        tokenizer=tokenize,\n",
    "                                        max_df=0.4)),\n",
    "                                ('tfidf', TfidfTransformer())\n",
    "                        ])),\n",
    "                        ('start_verb', StartingVerbExtractor())])),\n",
    "                ('clf', MultiOutputClassifier(\n",
    "                        AdaBoostClassifier(\n",
    "                                n_estimators=51,\n",
    "                                learning_rate=1.2\n",
    "                        ))\n",
    "                 )])\n",
    "\n",
    "        return ada_ppl\n",
    "\n",
    "    elif clf_type == \"XG\":\n",
    "\n",
    "        # XGBoost models are much more computationally expensive than AdaBoost\n",
    "        xgb_ppl = Pipeline([\n",
    "                ('feats', FeatureUnion([\n",
    "                        ('text_ppl', Pipeline([\n",
    "                                ('vect', CountVectorizer(\n",
    "                                        tokenizer=tokenize,\n",
    "                                        max_df=0.75)),\n",
    "                                ('tfidf', TfidfTransformer())\n",
    "                        ])),\n",
    "                        ('start_verb', StartingVerbExtractor())])),\n",
    "                ('clf', MultiOutputClassifier(\n",
    "                        XGBClassifier(\n",
    "                                colsample_bytree=1.0,\n",
    "                                gamma=5.0,\n",
    "                                learning_rate=0.5,\n",
    "                                min_child_weight=1,\n",
    "                                subsample=1.0\n",
    "                        ))\n",
    "                 )])\n",
    "\n",
    "        return xgb_ppl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimize updated AdaBoost pipeline."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ada_ppl = build_ppl('Ada')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ada_params = {'feats__text_ppl__vect__max_df': (0.3, 0.4, 0.5),\n",
    "              'clf__estimator__n_estimators': range(50, 54, 1),\n",
    "              'clf__estimator__learning_rate': np.arange(1.2, 2.0, 0.2)\n",
    "              }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ada_gs = GridSearchCV(\n",
    "        ada_ppl,\n",
    "        param_grid=ada_params,\n",
    "        scoring=scorer,\n",
    "        n_jobs=12,\n",
    "        verbose=7\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ada_gs.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimize updated XGBoost pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_ppl = build_ppl('XG')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_params = {'feats__text_ppl__vect__max_df': (0.3, 0.4, 0.5),\n",
    "              'clf__estimator__max_depth': range(5, 10, 2),\n",
    "              'clf__estimator__learning_rate': [0.5, 0.75, 1.0],\n",
    "              'clf__estimator__min_child_weight':np.arange(1, 4, 1),\n",
    "              'clf__estimator__gamma':np.arange(5, 7.5, 0.5)\n",
    "              }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_gs = GridSearchCV(\n",
    "        xgb_ppl,\n",
    "        param_grid=xgb_params,\n",
    "        scoring=scorer,\n",
    "        n_jobs=12,\n",
    "        verbose=7\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_gs.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_gs.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 导出模型为 pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, path):\n",
    "    pickle.dump(model, open(path, 'wb'))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_save(pipeline, \"model.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "使用资源 (Resources)文件里附带的模板文件编写脚本，运行上述步骤，创建一个数据库，并基于用户指定的新数据集输出一个模型。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "playground",
   "language": "python",
   "display_name": "Py3 (pa)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}